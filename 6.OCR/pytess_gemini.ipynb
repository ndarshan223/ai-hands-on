{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e80a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.7' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b14fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(im_path):\n",
    "    dpi = 80\n",
    "    im_data = plt.imread(im_path)\n",
    "    height, width  = im_data.shape[:2]\n",
    "    figsize = width / float(dpi), height / float(dpi)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    ax.axis('off')\n",
    "    ax.imshow(im_data, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e6437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(img):\n",
    "    text = pytesseract.image_to_string(img, lang='eng')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92714514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_with_gemini(extracted_text, api_key):\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert invoice data extraction system. Analyze the following raw invoice text.\n",
    "    Your task is to extract all line-item details, calculate the total item count, and\n",
    "    determine the final reconciled amount (sum of all item_amount values).\n",
    "\n",
    "    **INSTRUCTIONS:**\n",
    "    1.  Assume the entire text belongs to 'page_no': '1'.\n",
    "    2.  Extract the item name (Particulars), quantity (Qty.), rate (Rate), and amount (Amount)\n",
    "        for every line item in the bill's table. If quantity or rate are not mentioned, use 0.0\n",
    "    3.  Set 'total_item_count' to the count of unique line items found.\n",
    "    4.  Set 'reconciled_amount' to the sum of all extracted 'item_amount' values.\n",
    "    5.  The final output MUST strictly adhere to the provided JSON schema.\n",
    "    6.  Include a 'token_usage' object with 'total_tokens', 'input_tokens', and 'output_tokens'.\n",
    "\n",
    "    **REQUIRED JSON SCHEMA:**\n",
    "    The final output must be a single JSON object (the response body for the API):\n",
    "    {{\n",
    "      \"is_success\": true,\n",
    "      \"data\": {{\n",
    "        \"pagewise_line_items\": [\n",
    "          {{\n",
    "            \"page_no\": \"1\",\n",
    "            \"bill_items\": [\n",
    "              {{\n",
    "                \"item_name\": \"...\",\n",
    "                \"item_amount\": 0.00,\n",
    "                \"item_rate\": 0.00,\n",
    "                \"item_quantity\": 0.00\n",
    "              }}\n",
    "              // ... continue for all line items\n",
    "            ]\n",
    "          }}\n",
    "        ],\n",
    "        \"total_item_count\": 0,\n",
    "        \"reconciled_amount\": 0.00,\n",
    "        \"token_usage\": {{\n",
    "          \"total_tokens\": 0,\n",
    "          \"input_tokens\": 0,\n",
    "          \"output_tokens\": 0\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "\n",
    "    Invoice text:\n",
    "    ---\n",
    "    {extracted_text}\n",
    "    ---\n",
    "\n",
    "    Return ONLY the valid JSON object, without any surrounding markdown, backticks, or explanation. The output should strictly not contain any characters that are not part of the json syntax\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33919b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51002416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response):\n",
    "    candidate = response.candidates[0]\n",
    "    content = candidate.content\n",
    "    raw_json_string = content.parts[0].text\n",
    "    cleaned_json_string = re.sub(r'```json|```', '', raw_json_string, flags=re.IGNORECASE).strip()\n",
    "    final_json_data = json.loads(cleaned_json_string)\n",
    "    \n",
    "    # Extract actual token usage from Gemini response\n",
    "    if hasattr(response, 'usage_metadata'):\n",
    "        token_usage = {\n",
    "            \"input_tokens\": response.usage_metadata.prompt_token_count,\n",
    "            \"output_tokens\": response.usage_metadata.candidates_token_count,\n",
    "            \"total_tokens\": response.usage_metadata.total_token_count\n",
    "        }\n",
    "        # Update the token_usage in the response data\n",
    "        if 'data' in final_json_data and 'token_usage' in final_json_data['data']:\n",
    "            final_json_data['data']['token_usage'] = token_usage\n",
    "    \n",
    "    return json.dumps(final_json_data)  # Return as formatted JSON string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a95da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://becominghuman.ai/how-to-automatically-deskew-straighten-a-text-image-using-opencv-a0c30aed83df\n",
    "import numpy as np\n",
    "\n",
    "def getSkewAngle(cvImage) -> float:\n",
    "    # Prep image, copy, convert to gray scale, blur, and threshold\n",
    "    newImage = cvImage.copy()\n",
    "    gray = cv.cvtColor(newImage, cv.COLOR_BGR2GRAY)\n",
    "    blur = cv.GaussianBlur(gray, (9, 9), 0)\n",
    "    thresh = cv.threshold(blur, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU)[1]\n",
    "\n",
    "    # Apply dilate to merge text into meaningful lines/paragraphs.\n",
    "    # Use larger kernel on X axis to merge characters into single line, cancelling out any spaces.\n",
    "    # But use smaller kernel on Y axis to separate between different blocks of text\n",
    "    kernel = cv.getStructuringElement(cv.MORPH_RECT, (30, 5))\n",
    "    dilate = cv.dilate(thresh, kernel, iterations=2)\n",
    "\n",
    "    # Find all contours\n",
    "    contours, hierarchy = cv.findContours(dilate, cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key = cv.contourArea, reverse = True)\n",
    "    for c in contours:\n",
    "        rect = cv.boundingRect(c)\n",
    "        x,y,w,h = rect\n",
    "        cv.rectangle(newImage,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "\n",
    "    # Find largest contour and surround in min area box\n",
    "    largestContour = contours[0]\n",
    "    print (len(contours))\n",
    "    minAreaRect = cv.minAreaRect(largestContour)\n",
    "    cv.imwrite(\"temp/boxes.jpg\", newImage)\n",
    "    # Determine the angle. Convert it to the value that was originally used to obtain skewed image\n",
    "    angle = minAreaRect[-1]\n",
    "    if angle < -45:\n",
    "        angle = 90 + angle\n",
    "    return -1.0 * angle\n",
    "# Rotate the image around its center\n",
    "def rotateImage(cvImage, angle: float):\n",
    "    newImage = cvImage.copy()\n",
    "    (h, w) = newImage.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv.getRotationMatrix2D(center, angle, 1.0)\n",
    "    newImage = cv.warpAffine(newImage, M, (w, h), flags=cv.INTER_CUBIC, borderMode=cv.BORDER_REPLICATE)\n",
    "    return newImage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e679d5a",
   "metadata": {},
   "source": [
    "### Sample 2: Pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"sample_data/sample_2.png\"\n",
    "img = cv.imread(image_path)\n",
    "extracted_text = extract_text_from_image(img)\n",
    "display(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fedf263",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = structure_with_gemini(extracted_text, api_key)\n",
    "output = format_response(response)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad2549",
   "metadata": {},
   "source": [
    "## Sample 3: Final Bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f360091",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"sample_data/sample_3.png\"\n",
    "img = cv.imread(image_path)\n",
    "extracted_text = extract_text_from_image(img)\n",
    "display(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf68e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = structure_with_gemini(extracted_text, api_key)\n",
    "output = format_response(response)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be2bd3",
   "metadata": {},
   "source": [
    "### Sample 1: Detail bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"sample_data/sample_1.png\"\n",
    "img = cv.imread(image_path)\n",
    "extracted_text = extract_text_from_image(img)\n",
    "display(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fdd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = structure_with_gemini(extracted_text, api_key)\n",
    "output = format_response(response)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734fbab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://hackrx.blob.core.windows.net/assets/datathon-IIT/sample_3.png?sv=2025-07-05&spr=https&st=2025-11-24T14%3A24%3A39Z&se=2026-11-25T14%3A24%3A00Z&sr=b&sp=r&sig=egKAmIUms8H5f3kgrGXKvcfuBVlQp0Qc2tsfxdvRgUY%3D\"\n",
    "\n",
    "# Step 1: Download image\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Convert to NumPy array\n",
    "img_array = np.frombuffer(response.content, np.uint8)\n",
    "\n",
    "# Step 3: Decode using OpenCV\n",
    "img = cv.imdecode(img_array, cv.IMREAD_COLOR)\n",
    "\n",
    "# Display image\n",
    "cv.imshow(\"Image\", img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
